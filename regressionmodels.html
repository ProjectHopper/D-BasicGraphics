
\chapter{Linear Regression}
\section{Simple Linear Regression}

We start with a scatter diagram between two variables as before. This time, we want
to know what line will best fit the data.

The theory we learn here assumes we are going to use a straight line, and not a curve of any kind, though in some disciplines (physics or finance, for example) a curve would be more appropriate.

We have to find the line of best fit. Before we can do this, we must assume that one
variable is dependent on the other. By convention we call the dependent variable y
and the independent variable x. We have to work out the slope of the line, and the
point at which it cuts the y axis.

Again, by convention, we call these values and respectively for the population.

The basic model is therefore given as follows.

The model of a random variable Y, the dependent variable, which is related to random
variable X, the independent (or predictor or explanatory) variable by the equation:
\begin{equation}
Y = \alpha + \beta X + \epsilon
\end{equation}

where $\alpha$ and $\beta$ are constants and $\epsilon ~ N ( 0,\sigma^2)$ , a random error term. The
coefficients $\alpha$ and $\beta$  are theoretical values and can only be estimated from sample data.

The estimates are generally written as $a$ and $b$.

Given a sample of bivariate data, $(x_1,y_1),(x_2,y_2) ,.........., (x_n,y_n)$, $a$ and $b$ can be
estimated. To fit a line to some data as in this case, an objective must be chosen to
define which straight line best describes the data. The most common objective is to
minimise the sum of the squared distance between the observed value of $y_i$
and the corresponding predicted value $\hat{y}_i$.

The estimated least squares regression line is written as:
$y = a + b\times x$
We can derive the formulae for $b$ and $a$.



The line is called the sample regression line of y on x.

The following example demonstrates the calculation of a and b and the use of the resultant
equation to estimate y for a given x.

\subsection{Ordinary least squares}
Ordinary least squares (OLS) is a technique for estimating the unknown parameters in a linear regression model. This method minimizes the sum of squared distances between the observed responses in a set of data, and the fitted responses from the regression model.

\subsection{Regression example}
A study was made by a retailer to determine the relation between weekly advertising
expenditure and sales (in thousands of pounds). Find the equation of a regression line
to predict weekly sales from advertising. Estimate weekly sales when advertising
costs are £35,000.

Adv. Costs(in £000) 40 20 25 20 30 50 40 20 50 40 25 50

Sales (in £000) 385 400 395 365 475 440 490 420 560 525 480 510



\subsection{example}
Concentration (ng/ml) 0 5 10 15 20 25 30
Absorbance 0.003 0.127 0.251 0.390 0.498 0.625 0.763

\begin{verbatim}
# DO A FULL LINEAR REGRESSION ANALYSIS ON THE DATA

>concentration=c(0,5,10,15,20,25,30)
>absorbance=c(0.03,0.127,0.251,0.390,0.498,0.625,0.763)
>regr=lm(absorbance~concentration)
# READ AS; ABSORBANCE DEPENDENT ON CONCENTRATION
>summary(regr)
\end{verbatim}

This output from this code is as follows:
\begin{verbatim}
Call:
lm(formula = absorbance ~ concentration)

Residuals:
        1         2         3         4         5         6         7
 0.015357 -0.010571 -0.009500  0.006571 -0.008357 -0.004286  0.010786

Coefficients:
               Estimate Std. Error t value Pr(>|t|)
(Intercept)   0.0146429  0.0079787   1.835    0.126
concentration 0.0245857  0.0004426  55.551 3.58e-08 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

Residual standard error: 0.01171 on 5 degrees of freedom
Multiple R-squared: 0.9984,     Adjusted R-squared: 0.9981
F-statistic:  3086 on 1 and 5 DF,  p-value: 3.576e-08

\end{verbatim}


\begin{itemize}
\item Estimation of Slope = 0.0251643 \item  Standard error of the
estimation of the slope is 0.0002656 \item Estimation of Intercept
= 0.0021071 \item Standard error of the estimation of the
intercept is 0.0047874 \item Degrees-of-Freedom1 = 7-2 = 5  The
critical values for testing is are -2.57 and 2.57 since the area
under the Students t distribution
curve with 5 degrees-of-freedom outside this range is 5%
\item p-value for intercept is 67.8\% implying that it is not
significantly different from zero \item The p-value for the
intercept is the area under the Students t-distribution curve
with 5 degrees-of-freedom outside the range of [-0.44,0.44]. \item
p-value for slope is less than 5\% implying that it is
significantly different from zero \item The p-value for the slope
is the area under the Students t-distribution curve with 5
degrees of- freedom outside the range of [-94.76,94.76].
\end{itemize}

\subsection{Regression example}

A survey was conducted in 9 areas of the USA to investigate the relationship between
divorce rate (y) and residential mobility (x). Divorce rates in the annual number per 1000 in the population
and the residential mobility is measured by the percentage of the population that moved house in the last
five years.



\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
  \hline
Area & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9  \\
x & 40 & 38 & 46 & 49 & 47 & 43 & 51 & 57 & 55\\
y & 3.9 & 3.4 & 5.2 & 4.8 & 5.6 & 5.8 & 6.6 & 7.6 & 5.8\\
  \hline
\end{tabular}

\begin{itemize}
\item Check that the following statements are correct.

\begin{itemize}
\item sum of x data = 426
\item sum of squares of x data = 20494
\item sum of y data = 48.7
\item sum of squares of y data = 276.81
\item sum of products of x and y data = 2361
\end{itemize}

\item Derive the estimates for the slope and intercept of the regression line.
\item Estimate the divorce rate for areas that has a residential mobility of 39 and 60 respectively.
\item Which of these estimates is likely to be more accurate? Why?

\end{itemize}

\section{Regression}

The argument to lm is a model formula in which the tilde symbol
(~) should be read as ``described by.


This was seen several times earlier, both in connection with
boxplots and stripcharts and with the t and Wilcoxon tests.



\subsection{Multiple Linear Regression}
The \texttt{lm()} function handles much more
complicated models than simple linear regression. There can be many other things besides a dependent and a descriptive variable in a model formula.

A multiple linear regression analysis (which we discuss in Chapter
11) of, for example, y on x1, x2, and x3 is specified as $y ~ x1 +
x2 + x3$.


This is an F test for the hypothesis that the regression coefficient is zero. This test is not really interesting in a
simple linear regression analysis since it just duplicates information already givenit becomes more interesting when there is more than one explanatory variable.

\subsection{Regression}

\begin{verbatim}
> lm(short.velocity~blood.glucose)
\end{verbatim}

\section{Inference for Regression}
To determine the confidence interval for the slope we use the
following equation:
\begin{equation}
b \pm t_{1-\alpha/2,n-2} S.E.(b)
\end{equation}

\begin{itemize}
\item b = Estimation of Slope (0.0251643) \item S.E.(b) = Standard
Error of Slope(0.0002656) \item n = Sample Size (7) \item $\alpha$
= Alpha Value (5\%) \item $t_{1-\alpha/2,n-2}$ = Quantile Value
from Students t-distribution (2.570582)
\end{itemize}

\begin{equation}
(0.0251643) \pm (0.0002656)(2.570582) = [ 0.0245,0.0258 ]
\end{equation}


\subsection{Regression example}

In a medical experiment concerning 12 patients with a certain type of ear condition,
the following measurements were made for blood flow (y) and auricular pressure (x):

\begin{verbatim}
x<-c(8.5, 9.8, 10.8, 11.5, 11.2, 9.6, 10.1, 13.5, 14.2, 11.8, 8.7, 6.8)
y<-c(3 ,12, 10, 14, 8 ,7 ,9 ,13, 17, 10, 5 ,5)
\end{verbatim}


(Sx =126.5 Sxx =1,381.85 Sy =113 Syy =1251 Sxy =1272.2)


\begin{itemize}
\item Calculate the equation of the least-squares fitted regression line of blood flow
on auricular pressure.
\item Confirm the following values: Sx =126.5, Sxx =1381.85, Sy =113, Syy =1251, Sxy =1272.2.
\item Calculate the correlation coefficient.

\begin{verbatim}
> cor(x,y)
[1] 0.8521414
\end{verbatim}
\end{itemize}




\section{Regression}
Unweighted regression requires that the variability of the
residuals is constant over the measured range of values.
(This is called homoskedasticity).

Weighted regression does not have this requirement.
There may be differing variability over the range of values.
(This is called heteroskedasticity).

Weighted regression requires extra information on the standard deviations of the responses so as to compute the weights.

Unweighted regression doesnt need or use any information on the response standard deviations.

Weighted regression is preferable if heteroskedasticity evident in the data

(If there is not constant variance for the residuals over the range of values)

%------------------------------------------------------------------------------------------------%
\subsection{R square}
The model with the highest R2 and adjusted R2  is the preferable of all candidate models
The quadratic model is the preferable model in that case.






\section*{Polynomial Regression}
\begin{itemize}
\item Polynomial regression models are useful in situations where the analyst knows that \textit{\textbf{curvilinear effects}} are present in the response variable.
\item Polynomial models are also useful as approximating functions to unknown and possible very complex nonlinear relationship.
\end{itemize}
